{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca de √çndice e Consulta de documento\n",
    "\n",
    "**Autor:** Davi J. Leite Santos  \n",
    "**Vers√£o:** 0.0.3  \n",
    "**Data:** 25 de Abril de 2024  \n",
    "**Localiza√ß√£o:** Ribeir√£o das Neves, Minas Gerais - Brasil  \n",
    "\n",
    "## Contato\n",
    "- üè† **Endere√ßo:** Ribeir√£o das Neves, Minas Gerais - Brasil\n",
    "- üìß **Email:** davi.jls@outlook.com\n",
    "- üåê **LinkedIn:** davi-j-leite-santos\n",
    "- üåê **Website:** davijls.com.br\n",
    "\n",
    "## Principais Compet√™ncias\n",
    "- **Ciberseguran√ßa**\n",
    "- **Seguran√ßa da Informa√ß√£o**\n",
    "- **Opera√ß√µes de TI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza dos dados\n",
    "\n",
    "Nessa etapa ser√£o excluidas as stopworlds e transformadas os dados em UTF8 e retiradas os caracteres especias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista expandida de stopwords em portugu√™s\n",
    "stopwords = set([\n",
    "    \"a\", \"√†\", \"adeus\", \"agora\", \"ainda\", \"al√©m\", \"algmas\", \"alguns\", \"ali\", \"ambas\", \"ambos\",\n",
    "    \"ante\", \"antes\", \"ao\", \"aos\", \"apenas\", \"apoio\", \"ap√≥s\", \"aquela\", \"aquelas\", \"aquele\",\n",
    "    \"aqueles\", \"aqui\", \"aquilo\", \"as\", \"at√©\", \"atrav√©s\", \"cada\", \"c√°\", \"catorze\", \"cedo\",\n",
    "    \"cento\", \"certamente\", \"certeza\", \"cima\", \"cinq√ºenta\", \"cinco\", \"com\", \"como\", \"conselho\",\n",
    "    \"contra\", \"contudo\", \"da\", \"daquela\", \"daquelas\", \"daquele\", \"daqueles\", \"dar\", \"das\",\n",
    "    \"de\", \"dela\", \"delas\", \"dele\", \"deles\", \"demais\", \"dentro\", \"depois\", \"desde\", \"dessa\",\n",
    "    \"dessas\", \"desse\", \"desses\", \"desta\", \"destas\", \"deste\", \"destes\", \"deve\", \"devem\", \"dever√°\",\n",
    "    \"dez\", \"dezanove\", \"dezasseis\", \"dezassete\", \"dezoito\", \"dia\", \"diante\", \"disse\", \"disso\",\n",
    "    \"disto\", \"dito\", \"diz\", \"dizem\", \"do\", \"dois\", \"dos\", \"doze\", \"duas\", \"d√∫vida\", \"e\", \"√©\",\n",
    "    \"ela\", \"elas\", \"ele\", \"eles\", \"em\", \"embora\", \"enquanto\", \"ent√£o\", \"entre\", \"era\", \"eram\",\n",
    "    \"√©ramos\", \"√©s\", \"essa\", \"essas\", \"esse\", \"esses\", \"esta\", \"est√°\", \"estamos\", \"est√£o\",\n",
    "    \"estar\", \"estas\", \"est√°s\", \"estava\", \"estavam\", \"est√°vamos\", \"este\", \"estes\", \"esteve\",\n",
    "    \"estive\", \"estivemos\", \"estiveram\", \"estiveste\", \"estivestes\", \"estou\", \"eu\", \"exemplo\",\n",
    "    \"fa√ßo\", \"far√°\", \"favor\", \"faz\", \"fazeis\", \"fazem\", \"fazemos\", \"fazer\", \"fazes\", \"feita\",\n",
    "    \"feitas\", \"feito\", \"feitos\", \"fez\", \"fim\", \"final\", \"foi\", \"fomos\", \"for\", \"fora\", \"foram\",\n",
    "    \"forma\", \"foste\", \"fostes\", \"fui\", \"geral\", \"grande\", \"grandes\", \"grupo\", \"h√°\", \"haja\",\n",
    "    \"hajam\", \"h√£o\", \"havemos\", \"havia\", \"hei\", \"hoje\", \"hora\", \"horas\", \"houve\", \"houvemos\",\n",
    "    \"houveram\", \"houverei\", \"houveremos\", \"houveria\", \"houveriam\", \"houvermos\", \"houvesse\",\n",
    "    \"houvessem\", \"isso\", \"isto\", \"j√°\", \"la\", \"l√°\", \"lhe\", \"lhes\", \"lo\", \"logo\", \"longe\",\n",
    "    \"lugar\", \"maior\", \"maioria\", \"mais\", \"mal\", \"mas\", \"m√°ximo\", \"me\", \"melhor\", \"mesma\",\n",
    "    \"mesmas\", \"mesmo\", \"mesmos\", \"meu\", \"meus\", \"minha\", \"minhas\", \"momento\", \"muita\", \"muitas\",\n",
    "    \"muito\", \"muitos\", \"na\", \"nada\", \"n√£o\", \"naquela\", \"naquelas\", \"naquele\", \"naqueles\", \"nas\",\n",
    "    \"nem\", \"nenhum\", \"nessa\", \"nessas\", \"nesse\", \"nesses\", \"nesta\", \"nestas\", \"neste\", \"nestes\",\n",
    "    \"ningu√©m\", \"n√≠vel\", \"no\", \"noite\", \"n√≥s\", \"nome\", \"nos\", \"nossa\", \"nossas\", \"nosso\",\n",
    "    \"nossos\", \"nova\", \"novas\", \"nove\", \"novo\", \"novos\", \"num\", \"numa\", \"n√∫mero\", \"nunca\", \"o\",\n",
    "    \"obra\", \"obrigada\", \"obrigado\", \"oitava\", \"oitavo\", \"oito\", \"onde\", \"ontem\", \"onze\", \"os\",\n",
    "    \"ou\", \"outra\", \"outras\", \"outro\", \"outros\", \"para\", \"parece\", \"parte\", \"partir\", \"paucas\",\n",
    "    \"pela\", \"pelas\", \"pelo\", \"pelos\", \"perante\", \"perto\", \"pode\", \"pude\", \"podem\", \"poder\",\n",
    "    \"poder√°\", \"podia\", \"pois\", \"p√µe\", \"p√µem\", \"ponto\", \"pontos\", \"por\", \"porque\", \"porqu√™\",\n",
    "    \"pouca\", \"poucas\", \"pouco\", \"poucos\", \"primeira\", \"primeiras\", \"primeiro\", \"primeiros\",\n",
    "    \"promeiro\", \"pr√≥pria\", \"pr√≥prias\", \"pr√≥prio\", \"pr√≥prios\", \"pr√≥xima\", \"pr√≥ximas\", \"pr√≥ximo\",\n",
    "    \"pr√≥ximos\", \"pude\", \"p√¥de\", \"quais\", \"qu√°is\", \"qual\", \"qualquer\", \"quando\", \"quanto\",\n",
    "    \"quarta\", \"quarto\", \"quatro\", \"que\", \"qu√™\", \"quem\", \"quer\", \"quereis\", \"querem\", \"queremas\",\n",
    "    \"queres\", \"quero\", \"quest√£o\", \"quieto\", \"quinze\", \"qu√©n\", \"qu√©n\", \"rela√ß√£o\", \"sabe\", \"sabem\",\n",
    "    \"s√£o\", \"se\", \"segunda\", \"segundo\", \"sei\", \"seis\", \"seja\", \"sejam\", \"sem\", \"sempre\", \"sendo\",\n",
    "    \"ser\", \"ser√°\", \"ser√£o\", \"seria\", \"seriam\", \"sete\", \"s√©tima\", \"s√©timo\", \"seu\", \"seus\", \"seus\",\n",
    "    \"s√≥\", \"sob\", \"sobre\", \"sois\", \"somente\", \"somos\", \"sou\", \"sua\", \"suas\", \"tal\", \"talvez\",\n",
    "    \"tamb√©m\", \"tampouco\", \"tanta\", \"tantas\", \"tanto\", \"t√£o\", \"tarde\", \"te\", \"tem\", \"t√™m\", \"temos\",\n",
    "    \"tendes\", \"tendo\", \"tenha\", \"tenham\", \"tenho\", \"tens\", \"tentar\", \"tentaram\", \"tente\", \"tentei\",\n",
    "    \"ter\", \"ter√°\", \"ter√£o\", \"terei\", \"teremos\", \"teria\", \"teriam\", \"termos\", \"teu\", \"teus\",\n",
    "    \"teve\", \"tive\", \"tivemos\", \"tiveram\", \"tivera\", \"tiveram\", \"tiveste\", \"tivestes\", \"toda\",\n",
    "    \"todas\", \"todavia\", \"todo\", \"todos\", \"trabalho\", \"tr√™s\", \"treze\", \"tu\", \"tua\", \"tuas\",\n",
    "    \"tudo\", \"√∫ltima\", \"√∫ltimas\", \"√∫ltimo\", \"√∫ltimos\", \"um\", \"uma\", \"umas\", \"uns\", \"usa\", \"usar\",\n",
    "    \"valor\", \"veja\", \"vem\", \"vens\", \"ver\", \"vez\", \"vezes\", \"vindo\", \"vinte\", \"voc√™\", \"voc√™s\",\n",
    "    \"vos\", \"vossa\", \"vossas\", \"vosso\", \"vossos\", \"zero\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para limpar texto, removendo caracteres especiais, stopwords e convertendo para UTF-8\n",
    "def clean_text(text):\n",
    "    # Remover caracteres especiais e substituir por espa√ßos\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Converter para min√∫sculas\n",
    "    text = text.lower()\n",
    "    # Remover stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    # Garantir que o texto esteja em UTF-8\n",
    "    return text.encode('utf-8').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para limpar vocabul√°rio e √≠ndice\n",
    "def clean_vocab_and_index(vocab, index):\n",
    "    cleaned_vocab = {}\n",
    "    cleaned_index = {}\n",
    "    for word, word_id in vocab.items():\n",
    "        cleaned_word = clean_text(word)\n",
    "        if cleaned_word not in cleaned_vocab:\n",
    "            cleaned_vocab[cleaned_word] = word_id\n",
    "        cleaned_word_id = cleaned_vocab[cleaned_word]\n",
    "        \n",
    "        if cleaned_word_id not in cleaned_index:\n",
    "            cleaned_index[cleaned_word_id] = index[str(word_id)]\n",
    "        else:\n",
    "            for doc_id, positions in index[str(word_id)].items():\n",
    "                if doc_id not in cleaned_index[cleaned_word_id]:\n",
    "                    cleaned_index[cleaned_word_id][doc_id] = positions\n",
    "                else:\n",
    "                    cleaned_index[cleaned_word_id][doc_id].extend(positions)\n",
    "    \n",
    "    return cleaned_vocab, cleaned_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para limpar o formato de tupla e lista\n",
    "def clean_tuple_and_list_formats(tuple_format, list_format):\n",
    "    cleaned_tuple_format = [(clean_text(str(word_id)), doc_id, positions) for word_id, doc_id, positions in tuple_format]\n",
    "    cleaned_list_format = [{'word_id': clean_text(str(item['word_id'])), 'doc_id': item['doc_id'], 'positions': item['positions']} for item in list_format]\n",
    "    \n",
    "    return cleaned_tuple_format, cleaned_list_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal para carregar, limpar e salvar os dados\n",
    "def clean_data(index_geral_file, vocab_geral_file, tuple_format_file, list_geral_file):\n",
    "    # Carregar os arquivos\n",
    "    with open(index_geral_file, 'r', encoding='utf-8') as f:\n",
    "        index_geral = json.load(f)\n",
    "    with open(vocab_geral_file, 'r', encoding='utf-8') as f:\n",
    "        vocab_geral = json.load(f)\n",
    "    with open(tuple_format_file, 'r', encoding='utf-8') as f:\n",
    "        tuple_format = json.load(f)\n",
    "    with open(list_geral_file, 'r', encoding='utf-8') as f:\n",
    "        list_geral = json.load(f)\n",
    "    \n",
    "    # Limpar vocabul√°rio e √≠ndice\n",
    "    cleaned_vocab, cleaned_index = clean_vocab_and_index(vocab_geral, index_geral)\n",
    "    \n",
    "    # Limpar formatos de tupla e lista\n",
    "    cleaned_tuple_format, cleaned_list_format = clean_tuple_and_list_formats(tuple_format, list_geral)\n",
    "    \n",
    "    # Salvar os dados limpos\n",
    "    with open(index_geral_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_index, f, ensure_ascii=False, indent=4)\n",
    "    with open(vocab_geral_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_vocab, f, ensure_ascii=False, indent=4)\n",
    "    with open(tuple_format_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_tuple_format, f, ensure_ascii=False, indent=4)\n",
    "    with open(list_geral_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_list_format, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina os caminhos dos arquivos gerais\n",
    "index_geral_file = 'index_geral.json'\n",
    "vocab_geral_file = 'vocab_geral.json'\n",
    "tuple_format_file = 'tuple_format.json'\n",
    "list_geral_file = 'list_geral.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar os dados\n",
    "clean_data(index_geral_file, vocab_geral_file, tuple_format_file, list_geral_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise L√©xica e stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalando a biblioteca para fazer a analise L√©xica e stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Baixar os recursos necess√°rios do NLTK\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar o Stemmer para portugu√™s\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando as fun√ß√µes para a utiliza√ß√£o das analises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para an√°lise l√©xica e stemming\n",
    "def lexical_analysis_and_stemming(text):\n",
    "    # Tokeniza√ß√£o do texto em palavras\n",
    "    tokens = nltk.word_tokenize(text, language='portuguese')\n",
    "    # Aplicar stemming em cada token\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para aplicar an√°lise l√©xica e stemming em vocabul√°rio e √≠ndice\n",
    "def apply_lexical_analysis_and_stemming(vocab, index):\n",
    "    stemmed_vocab = {}\n",
    "    stemmed_index = {}\n",
    "\n",
    "    for word, word_id in vocab.items():\n",
    "        # An√°lise l√©xica e stemming\n",
    "        stemmed_word = ' '.join(lexical_analysis_and_stemming(word))\n",
    "        if stemmed_word not in stemmed_vocab:\n",
    "            stemmed_vocab[stemmed_word] = word_id\n",
    "        stemmed_word_id = stemmed_vocab[stemmed_word]\n",
    "\n",
    "        if str(word_id) in index:\n",
    "            if stemmed_word_id not in stemmed_index:\n",
    "                stemmed_index[stemmed_word_id] = index[str(word_id)]\n",
    "            else:\n",
    "                for doc_id, positions in index[str(word_id)].items():\n",
    "                    if doc_id not in stemmed_index[stemmed_word_id]:\n",
    "                        stemmed_index[stemmed_word_id][doc_id] = positions\n",
    "                    else:\n",
    "                        stemmed_index[stemmed_word_id][doc_id].extend(positions)\n",
    "    \n",
    "    return stemmed_vocab, stemmed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para aplicar an√°lise l√©xica e stemming no formato de tupla e lista\n",
    "def apply_lexical_analysis_and_stemming_to_formats(tuple_format, list_format):\n",
    "    stemmed_tuple_format = [(lexical_analysis_and_stemming(str(word_id))[0], doc_id, positions) for word_id, doc_id, positions in tuple_format]\n",
    "    stemmed_list_format = [{'word_id': lexical_analysis_and_stemming(str(item['word_id']))[0], 'doc_id': item['doc_id'], 'positions': item['positions']} for item in list_format]\n",
    "    \n",
    "    return stemmed_tuple_format, stemmed_list_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal para carregar, processar e salvar os dados com an√°lise l√©xica e stemming\n",
    "def process_data_with_lexical_analysis_and_stemming(index_geral_file, vocab_geral_file, tuple_format_file, list_geral_file):\n",
    "    # Carregar os arquivos\n",
    "    with open(index_geral_file, 'r', encoding='utf-8') as f:\n",
    "        index_geral = json.load(f)\n",
    "    with open(vocab_geral_file, 'r', encoding='utf-8') as f:\n",
    "        vocab_geral = json.load(f)\n",
    "    with open(tuple_format_file, 'r', encoding='utf-8') as f:\n",
    "        tuple_format = json.load(f)\n",
    "    with open(list_geral_file, 'r', encoding='utf-8') as f:\n",
    "        list_geral = json.load(f)\n",
    "    \n",
    "    # Aplicar an√°lise l√©xica e stemming em vocabul√°rio e √≠ndice\n",
    "    stemmed_vocab, stemmed_index = apply_lexical_analysis_and_stemming(vocab_geral, index_geral)\n",
    "    \n",
    "    # Aplicar an√°lise l√©xica e stemming nos formatos de tupla e lista\n",
    "    stemmed_tuple_format, stemmed_list_format = apply_lexical_analysis_and_stemming_to_formats(tuple_format, list_geral)\n",
    "    \n",
    "    # Salvar os dados processados\n",
    "    with open(index_geral_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stemmed_index, f, ensure_ascii=False, indent=4)\n",
    "    with open(vocab_geral_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stemmed_vocab, f, ensure_ascii=False, indent=4)\n",
    "    with open(tuple_format_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stemmed_tuple_format, f, ensure_ascii=False, indent=4)\n",
    "    with open(list_geral_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stemmed_list_format, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina os caminhos dos arquivos gerais\n",
    "index_geral_file = 'index_geral.json'\n",
    "vocab_geral_file = 'vocab_geral.json'\n",
    "tuple_format_file = 'tuple_format.json'\n",
    "list_geral_file = 'list_geral.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar os dados com an√°lise l√©xica e stemming\n",
    "process_data_with_lexical_analysis_and_stemming(index_geral_file, vocab_geral_file, tuple_format_file, list_geral_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa para fornece uma medi√ß√£o b√°sica de desempenho e espa√ßo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e vocabul√°rio\n",
    "def load_data(index_file, vocab_file):\n",
    "    with open(index_file, 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para buscar termos no √≠ndice\n",
    "def search_term(index, vocab, term):\n",
    "    stemmed_term = stemmer.stem(term)\n",
    "    if stemmed_term in vocab:\n",
    "        word_id = vocab[stemmed_term]\n",
    "        if str(word_id) in index:\n",
    "            return index[str(word_id)]\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para medir o tempo de pesquisa e o espa√ßo de indexa√ß√£o\n",
    "def measure_performance(index_file, vocab_file, search_terms, output_file):\n",
    "    index, vocab = load_data(index_file, vocab_file)\n",
    "    \n",
    "    # Medir tempo de pesquisa\n",
    "    search_times = []\n",
    "    for term in search_terms:\n",
    "        start_time = time.time()\n",
    "        search_term(index, vocab, term)\n",
    "        end_time = time.time()\n",
    "        search_times.append(end_time - start_time)\n",
    "    \n",
    "    avg_search_time = sum(search_times) / len(search_times)\n",
    "    \n",
    "    # Medir tamanho dos arquivos\n",
    "    index_size = os.path.getsize(index_file)\n",
    "    vocab_size = os.path.getsize(vocab_file)\n",
    "    total_size = index_size + vocab_size\n",
    "    \n",
    "    # Escrever resultados em um arquivo de texto\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Tempo m√©dio de pesquisa: {avg_search_time:.6f} segundos\\n\")\n",
    "        f.write(f\"Tamanho do arquivo de √≠ndice: {index_size} bytes\\n\")\n",
    "        f.write(f\"Tamanho do arquivo de vocabul√°rio: {vocab_size} bytes\\n\")\n",
    "        f.write(f\"Tamanho total dos arquivos: {total_size} bytes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os caminhos dos arquivos\n",
    "index_geral_file = 'index_geral.json'\n",
    "vocab_geral_file = 'vocab_geral.json'\n",
    "output_file = 'performance_metrics.txt'\n",
    "\n",
    "# Definir termos de pesquisa para teste\n",
    "search_terms = ['educa√ß√£o', 'linguagem', 'noticia', 'ideia', 'politica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir desempenho e salvar resultados\n",
    "measure_performance(index_geral_file, vocab_geral_file, search_terms, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
