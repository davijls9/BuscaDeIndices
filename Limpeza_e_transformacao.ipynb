{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca de √çndice e Consulta de documento\n",
    "\n",
    "**Autor:** Davi J. Leite Santos  \n",
    "**Vers√£o:** 0.0.3  \n",
    "**Data:** 25 de Abril de 2024  \n",
    "**Localiza√ß√£o:** Ribeir√£o das Neves, Minas Gerais - Brasil  \n",
    "\n",
    "## Contato\n",
    "- üè† **Endere√ßo:** Ribeir√£o das Neves, Minas Gerais - Brasil\n",
    "- üìß **Email:** davi.jls@outlook.com\n",
    "- üåê **LinkedIn:** davi-j-leite-santos\n",
    "- üåê **Website:** davijls.com.br\n",
    "\n",
    "## Principais Compet√™ncias\n",
    "- **Ciberseguran√ßa**\n",
    "- **Seguran√ßa da Informa√ß√£o**\n",
    "- **Opera√ß√µes de TI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sobre o codigo\n",
    "\n",
    "Este c√≥digo √© uma extens√£o avan√ßada para a cria√ß√£o, manipula√ß√£o e pesquisa de um √≠ndice invertido, incluindo recursos de pr√©-processamento de texto, como remo√ß√£o de stopwords, normaliza√ß√£o de caracteres, tokeniza√ß√£o e aplica√ß√£o de stemming em portugu√™s. Ele est√° integrado com opera√ß√µes de leitura e escrita JSON, e a implementa√ß√£o √© orientada para uma an√°lise de texto eficiente e recupera√ß√£o de documentos.\n",
    "\n",
    "### Componentes Principais do C√≥digo\n",
    "\n",
    "#### 1. Prepara√ß√£o e Depend√™ncias\n",
    "- Importa√ß√µes de m√≥dulos essenciais como `json`, `re` (para express√µes regulares), `nltk` (biblioteca de processamento de linguagem natural), `os` (para manipula√ß√£o de sistema de arquivos) e `time` (para medi√ß√£o de desempenho).\n",
    "- Download de recursos do NLTK necess√°rios para o processamento de texto, incluindo stopwords em portugu√™s e o stemmer `RSLPStemmer`.\n",
    "\n",
    "#### 2. Limpe leitura e escrita de dados JSON\n",
    "- **load_json** e **save_json**: Fun√ß√µes para carregar e salvar dados em formato JSON, garantindo que a codifica√ß√£o seja adequada para suportar caracteres especiais em UTF-8.\n",
    "\n",
    "#### 3. Limpeza e An√°lise L√©xica\n",
    "- **clean_text**: Remove caracteres especiais e stopwords, al√©m de converter o texto para min√∫sculas, preparando-o para uma an√°lise mais eficaz.\n",
    "- **lexical_analysis_and_stemming**: Aplica tokeniza√ß√£o, limpe you palavras desnecess√°rias e realiza stemming, transformando palavras em suas formas raiz para reduzir a complexidade do √≠ndice.\n",
    "\n",
    "#### 4. Transforma√ß√£o e Indexa√ß√£o\n",
    "- **apply_lexical_analysis_and_stemming**: Transforma vocabul√°rios e √≠ndices existentes com as opera√ß√µes de an√°lise l√©xica e stemming para criar vers√µes mais compactas e eficientes.\n",
    "- **process_data_with_lexical_analysis_and_stemming**: Processa os dados lendo o √≠ndice geral e o vocabul√°rio, aplicando as transforma√ß√µes e salvando os novos √≠ndices.\n",
    "\n",
    "#### 5. Funcionalidades de Busca\n",
    "- **search_query**: Realiza uma busca efetiva utilizando o √≠ndice e vocabul√°rio processados para encontrar documentos que contenham termos espec√≠ficos, mostrando como palavras processadas podem ser rapidamente mapeadas aos documentos originais.\n",
    "\n",
    "#### 6. Avalia√ß√£o de Desempenho\n",
    "- **measure_performance**: Avalia o desempenho do sistema medindo o tempo de resposta para consultas e o tamanho dos arquivos de √≠ndice e vocabul√°rio, permitindo uma avalia√ß√£o objetiva da efici√™ncia das otimiza√ß√µes implementadas.\n",
    "\n",
    "### Implementa√ß√£o e Uso\n",
    "\n",
    "O c√≥digo √© cuidadosamente projetado para ser robusto e eficiente, utilizando pr√°ticas de codifica√ß√£o que garantem a integridade dos dados e a facilidade de manuten√ß√£o. Ele oferece uma solu√ß√£o completa para a manipula√ß√£o de grandes conjuntos de dados de texto em portugu√™s, ideal para aplica√ß√µes que requerem indexa√ß√£o e recupera√ß√£o de informa√ß√µes textuais r√°pidas e precisas.\n",
    "\n",
    "Este sistema √© especialmente √∫til em contextos onde a precis√£o lexical e a rapidez na recupera√ß√£o de informa√ß√µes s√£o cruciais, como em motores de busca ou sistemas de gerenciamento de documentos digitais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Baixar recursos do NLTK\n",
    "nltk.download(\"rslp\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Inicializar o Stemmer para portugu√™s\n",
    "stemmer = RSLPStemmer()\n",
    "stop_words = set(stopwords.words(\"portuguese\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza dos dados\n",
    "\n",
    "Nessa etapa ser√£o excluidas as stopworlds e transformadas os dados em UTF8 e retiradas os caracteres especias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove caracteres especiais\n",
    "    text = text.lower()  # Converte para min√∫sculas\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def lexical_analysis_and_stemming(text):\n",
    "    # Limpa o texto\n",
    "    cleaned_text = clean_text(text)\n",
    "    # Tokeniza o texto em palavras\n",
    "    tokens = nltk.word_tokenize(cleaned_text, language=\"portuguese\")\n",
    "    # Remove stopwords e aplica stemming em cada token\n",
    "    stemmed_tokens = [\n",
    "        stemmer.stem(token) for token in tokens if token not in stop_words\n",
    "    ]\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "def apply_lexical_analysis_and_stemming(vocab, index):\n",
    "    stemmed_vocab = {}\n",
    "    stemmed_index = {}\n",
    "\n",
    "    for word, word_id in vocab.items():\n",
    "        # An√°lise l√©xica e stemming\n",
    "        stemmed_word = \" \".join(lexical_analysis_and_stemming(word))\n",
    "        if stemmed_word not in stemmed_vocab:\n",
    "            stemmed_vocab[stemmed_word] = word_id\n",
    "        stemmed_word_id = stemmed_vocab[stemmed_word]\n",
    "\n",
    "        if str(word_id) in index:\n",
    "            for doc_id, positions in index[str(word_id)].items():\n",
    "                if stemmed_word_id not in stemmed_index:\n",
    "                    stemmed_index[stemmed_word_id] = {}\n",
    "                if doc_id not in stemmed_index[stemmed_word_id]:\n",
    "                    stemmed_index[stemmed_word_id][doc_id] = positions\n",
    "                else:\n",
    "                    if isinstance(stemmed_index[stemmed_word_id][doc_id], list):\n",
    "                        stemmed_index[stemmed_word_id][doc_id].extend(positions)\n",
    "                    else:\n",
    "                        stemmed_index[stemmed_word_id][doc_id] = list(positions)\n",
    "\n",
    "    return stemmed_vocab, stemmed_index\n",
    "\n",
    "\n",
    "# Fun√ß√£o principal para carregar, processar e salvar os dados com an√°lise l√©xica e stemming\n",
    "def process_data_with_lexical_analysis_and_stemming(\n",
    "    index_geral_file, vocab_geral_file, cleaned_index_file, cleaned_vocab_file\n",
    "):\n",
    "    # Carregar os arquivos\n",
    "    index_geral = load_json(index_geral_file)\n",
    "    vocab_geral = load_json(vocab_geral_file)\n",
    "\n",
    "    # Aplicar an√°lise l√©xica e stemming em vocabul√°rio e √≠ndice\n",
    "    stemmed_vocab, stemmed_index = apply_lexical_analysis_and_stemming(\n",
    "        vocab_geral, index_geral\n",
    "    )\n",
    "\n",
    "    # Salvar os dados processados\n",
    "    save_json(stemmed_index, cleaned_index_file)\n",
    "    save_json(stemmed_vocab, cleaned_vocab_file)\n",
    "\n",
    "\n",
    "# Fun√ß√£o para realizar a pesquisa\n",
    "def search_query(query, vocab_geral_file, index_geral_file, sites_dir):\n",
    "    # Carregar vocabul√°rio e √≠ndice geral\n",
    "    vocab_geral = load_json(vocab_geral_file)\n",
    "    index_geral = load_json(index_geral_file)\n",
    "\n",
    "    # Processar a query\n",
    "    query_terms = lexical_analysis_and_stemming(query.lower())\n",
    "\n",
    "    # Obter IDs das palavras da query\n",
    "    word_ids = []\n",
    "    for term in query_terms:\n",
    "        if term in vocab_geral:\n",
    "            word_ids.append(vocab_geral[term])\n",
    "\n",
    "    # Recuperar documentos correspondentes\n",
    "    document_ids = set()\n",
    "    for word_id in word_ids:\n",
    "        if str(word_id) in index_geral:\n",
    "            document_ids.update(index_geral[str(word_id)].keys())\n",
    "\n",
    "    # Converter IDs dos documentos para nomes dos arquivos\n",
    "    files = sorted(os.listdir(sites_dir))\n",
    "    result_files = []\n",
    "    for doc_id in document_ids:\n",
    "        try:\n",
    "            result_files.append(files[int(doc_id)])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "\n",
    "    return result_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina os caminhos dos arquivos\n",
    "index_geral_file = \"index_geral.json\"\n",
    "vocab_geral_file = \"vocab_geral.json\"\n",
    "cleaned_index_file = \"cleaned_index_geral.json\"\n",
    "cleaned_vocab_file = \"cleaned_vocab_geral.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar os dados com an√°lise l√©xica e stemming\n",
    "process_data_with_lexical_analysis_and_stemming(\n",
    "    index_geral_file, vocab_geral_file, cleaned_index_file, cleaned_vocab_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos de uso:\n",
    "sites_dir = \"../Motor_de_busca-WebCrawler/sites_visitados/\"\n",
    "query = \"noticia\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = search_query(query, cleaned_vocab_file, cleaned_index_file, sites_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in result_files:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifica√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalando a biblioteca para fazer a analise L√©xica e stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e vocabul√°rio\n",
    "def load_data(index_file, vocab_file):\n",
    "    with open(index_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab\n",
    "\n",
    "\n",
    "# Fun√ß√£o para buscar termos no √≠ndice\n",
    "def search_term(index, vocab, term):\n",
    "    cleaned_term = clean_text(term)\n",
    "    stemmed_term = stemmer.stem(cleaned_term)\n",
    "    if stemmed_term in vocab:\n",
    "        word_id = vocab[stemmed_term]\n",
    "        if str(word_id) in index:\n",
    "            return index[str(word_id)]\n",
    "    return {}\n",
    "\n",
    "\n",
    "# Fun√ß√£o para medir o tempo de pesquisa e o espa√ßo de indexa√ß√£o\n",
    "def measure_performance(index_file, vocab_file, search_terms, output_file):\n",
    "    index, vocab = load_data(index_file, vocab_file)\n",
    "\n",
    "    # Medir tempo de pesquisa\n",
    "    search_times = []\n",
    "    for term in search_terms:\n",
    "        start_time = time.time()\n",
    "        search_term(index, vocab, term)\n",
    "        end_time = time.time()\n",
    "        search_times.append(end_time - start_time)\n",
    "\n",
    "    avg_search_time = sum(search_times) / len(search_times)\n",
    "\n",
    "    # Medir tamanho dos arquivos\n",
    "    index_size = os.path.getsize(index_file)\n",
    "    vocab_size = os.path.getsize(vocab_file)\n",
    "    total_size = index_size + vocab_size\n",
    "\n",
    "    # Escrever resultados em um arquivo de texto\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Tempo m√©dio de pesquisa: {avg_search_time:.6f} segundos\\n\")\n",
    "        f.write(f\"Tamanho do arquivo de √≠ndice: {index_size} bytes\\n\")\n",
    "        f.write(f\"Tamanho do arquivo de vocabul√°rio: {vocab_size} bytes\\n\")\n",
    "        f.write(f\"Tamanho total dos arquivos: {total_size} bytes\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os caminhos dos novos arquivos\n",
    "cleaned_index_geral_file = \"cleaned_index_geral.json\"\n",
    "cleaned_vocab_geral_file = \"cleaned_vocab_geral.json\"\n",
    "output_file = \"performance_metrics.txt\"\n",
    "\n",
    "# Definir termos de pesquisa para teste\n",
    "search_terms = [\"educa√ß√£o\", \"linguagem\", \"noticia\", \"ideia\", \"politica\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir desempenho e salvar resultados\n",
    "measure_performance(\n",
    "    cleaned_index_geral_file, cleaned_vocab_geral_file, search_terms, output_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
