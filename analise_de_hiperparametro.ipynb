{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca de √çndice e Consulta de documento\n",
    "\n",
    "**Autor:** Davi J. Leite Santos  \n",
    "**Vers√£o:** 0.0.3  \n",
    "**Data:** 25 de Abril de 2024  \n",
    "**Localiza√ß√£o:** Ribeir√£o das Neves, Minas Gerais - Brasil  \n",
    "\n",
    "## Contato\n",
    "- üè† **Endere√ßo:** Ribeir√£o das Neves, Minas Gerais - Brasil\n",
    "- üìß **Email:** davi.jls@outlook.com\n",
    "- üåê **LinkedIn:** davi-j-leite-santos\n",
    "- üåê **Website:** davijls.com.br\n",
    "\n",
    "## Principais Compet√™ncias\n",
    "- **Ciberseguran√ßa**\n",
    "- **Seguran√ßa da Informa√ß√£o**\n",
    "- **Opera√ß√µes de TI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre o codigo\n",
    "Este c√≥digo Python apresenta um script robusto e detalhado desenvolvido para criar e analisar um √≠ndice de documentos com foco na otimiza√ß√£o do desempenho da busca de termos. O processo de indexa√ß√£o de texto e recupera√ß√£o de termos √© fundamentado no uso de \"stemming\" e \"chunking\" (segmenta√ß√£o de texto). O c√≥digo √© parte de um projeto pessoal de Davi J. Leite Santos sobre ciberseguran√ßa e opera√ß√µes de TI, contendo informa√ß√µes t√©cnicas sobre a manipula√ß√£o de grandes conjuntos de dados de texto.\n",
    "\n",
    "Detalhamento das Componentes Principais do C√≥digo\n",
    "## 1. Importa√ß√µes e Configura√ß√µes Iniciais\n",
    "O c√≥digo inicia com a importa√ß√£o de m√≥dulos necess√°rios como json para manipula√ß√£o de dados JSON, re para express√µes regulares, os para intera√ß√µes com o sistema operacional, time para medi√ß√£o de tempo e nltk, uma biblioteca de processamento de linguagem natural. Al√©m disso, o c√≥digo baixa os recursos do NLTK necess√°rios para \"stemming\" e tokeniza√ß√£o.\n",
    "\n",
    "## 2. Defini√ß√£o de Fun√ß√µes\n",
    "- load_index: Carrega um √≠ndice e um vocabul√°rio previamente salvos em arquivos JSON.\n",
    "- create_chunks: Divide um texto em segmentos de tamanho definido.\n",
    "- search_term: Busca um termo no √≠ndice ap√≥s convert√™-lo para sua forma raiz (\"stem\") usando o RSLPStemmer para o Portugu√™s.\n",
    "- save_progress: Salva o progresso de alguma opera√ß√£o em um arquivo de texto.\n",
    "- process_file: Processa um arquivo de texto dividindo-o em chunks, tokenizando-o e atualizando o √≠ndice e o vocabul√°rio com novas entradas.\n",
    "- create_inverted_index_with_chunks: Cria um √≠ndice invertido com a possibilidade de configurar o tamanho dos chunks.\n",
    "- measure_search_time_with_different_chunk_sizes: Mede o tempo de busca em um √≠ndice para avaliar o desempenho baseado em diferentes tamanhos de chunks.\n",
    "- index_documents_with_different_chunk_sizes: Cria √≠ndices segmentados com base em diversos tamanhos de chunks para avaliar o desempenho de armazenamento e busca.\n",
    "- analyze_hyperparameters: Executa a fun√ß√£o de indexa√ß√£o com diferentes configura√ß√µes de tamanho de chunk e documenta os resultados, incluindo o tempo m√©dio de busca e o espa√ßo de armazenamento usado.\n",
    "## 3. Execu√ß√£o da An√°lise de Hiperpar√¢metros\n",
    "O c√≥digo ent√£o configura e executa a fun√ß√£o analyze_hyperparameters, passando os diret√≥rios de arquivos, termos de busca, tamanhos de chunks desejados, e os caminhos para arquivos de sa√≠da e progresso. Esta fun√ß√£o √© a principal executora que desencadeia todo o processo de an√°lise de desempenho de indexa√ß√£o e busca.\n",
    "\n",
    "# Conclus√£o\n",
    "Este c√≥digo √© uma ferramenta sofisticada e detalhada para testar e analisar o desempenho de sistemas de indexa√ß√£o de documentos, focando principalmente no impacto de diferentes estrat√©gias de segmenta√ß√£o de dados. Ele integra v√°rias pr√°ticas avan√ßadas de engenharia de software, como modulariza√ß√£o de c√≥digo, reuso de fun√ß√µes e escrita eficiente e segura de dados. Ele exemplifica bem como scripts Python podem ser empregados em tarefas complexas de processamento de dados e an√°lise de desempenho em campos como a ciberseguran√ßa e TI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Baixar os recursos necess√°rios do NLTK\n",
    "nltk.download(\"rslp\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Inicializar o Stemmer para portugu√™s\n",
    "stemmer = RSLPStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun√ß√µes de cria√ß√£o de chunk para a analise de granularidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e o vocabul√°rio\n",
    "def load_index(index_file, vocab_file):\n",
    "    with open(index_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar chunks de um documento\n",
    "def create_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [\n",
    "        \" \".join(words[i : i + chunk_size]) for i in range(0, len(words), chunk_size)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para buscar termos no √≠ndice\n",
    "def search_term(index, vocab, term):\n",
    "    stemmed_term = stemmer.stem(term)\n",
    "    if stemmed_term in vocab:\n",
    "        word_id = vocab[stemmed_term]\n",
    "        if str(word_id) in index:\n",
    "            return index[str(word_id)]\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para salvar o progresso em um arquivo\n",
    "def save_progress(progress_file, progress):\n",
    "    with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Progress:\\n\")\n",
    "        for key, value in progress.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e o vocabul√°rio\n",
    "def load_index(index_file, vocab_file):\n",
    "    with open(index_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para simular a cria√ß√£o de arquivos de √≠ndice e vocabul√°rio\n",
    "def create_temp_files(index_file, vocab_file, chunk_size):\n",
    "    # Criar dados fict√≠cios para o √≠ndice e vocabul√°rio\n",
    "    index_data = {f\"word{chunk_size}\": [chunk_size, chunk_size + 1]}\n",
    "    vocab_data = {f\"word{chunk_size}\": chunk_size}\n",
    "\n",
    "    # Salvar dados fict√≠cios nos arquivos\n",
    "    with open(index_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(index_data, f)\n",
    "    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para processar arquivo com granularidade ajust√°vel\n",
    "def process_file(file_path, chunk_size, doc_id, vocab, index):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Arquivo n√£o encontrado: {file_path}\")\n",
    "        return\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    chunks = create_chunks(text, chunk_size)\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = nltk.word_tokenize(chunk, language=\"portuguese\")\n",
    "        for pos, token in enumerate(tokens):\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token not in vocab:\n",
    "                vocab[stemmed_token] = len(vocab)\n",
    "            word_id = vocab[stemmed_token]\n",
    "            if word_id not in index:\n",
    "                index[word_id] = {}\n",
    "            if doc_id not in index[word_id]:\n",
    "                index[word_id][doc_id] = []\n",
    "            index[word_id][doc_id].append((chunk_id, pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index_with_chunks(files, chunk_size):\n",
    "    index = {}\n",
    "    vocab = {}\n",
    "    vocab_id = 0\n",
    "    doc_id = 0\n",
    "\n",
    "    for file_path in files:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.read().split()\n",
    "\n",
    "        chunk_id = 0\n",
    "        while chunk_id * chunk_size < len(words):\n",
    "            chunk = words[chunk_id * chunk_size : (chunk_id + 1) * chunk_size]\n",
    "            for pos, word in enumerate(chunk):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = vocab_id\n",
    "                    vocab_id += 1\n",
    "                term_id = vocab[word]\n",
    "                if term_id not in index:\n",
    "                    index[term_id] = {}\n",
    "                if doc_id not in index[term_id]:\n",
    "                    index[term_id][doc_id] = []\n",
    "                index[term_id][doc_id].append(pos + chunk_id * chunk_size)\n",
    "            chunk_id += 1\n",
    "        doc_id += 1\n",
    "\n",
    "    return index, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para simular a medi√ß√£o do tempo de busca com diferentes tamanhos de chunk\n",
    "def measure_search_time_with_different_chunk_sizes(index, vocab, search_terms):\n",
    "    total_search_time = 0.0\n",
    "    num_searches = len(search_terms)\n",
    "    for term in search_terms:\n",
    "        start_time = time.time()\n",
    "        _ = index.get(term, [])\n",
    "        total_search_time += time.time() - start_time\n",
    "\n",
    "    avg_search_time = total_search_time / num_searches\n",
    "    return avg_search_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para simular a indexa√ß√£o de documentos com diferentes tamanhos de chunk\n",
    "def index_documents_with_different_chunk_sizes(files_dir, chunk_sizes):\n",
    "    index_file, vocab_file = files_dir\n",
    "    index, vocab = load_index(index_file, vocab_file)\n",
    "\n",
    "    results = []\n",
    "    for chunk_size in chunk_sizes:\n",
    "        chunked_index = {\n",
    "            k: v[:chunk_size] if isinstance(v, list) else v for k, v in index.items()\n",
    "        }\n",
    "        chunked_vocab = {k: v for k, v in vocab.items() if len(k) <= chunk_size}\n",
    "\n",
    "        index_size = len(json.dumps(chunked_index).encode(\"utf-8\"))\n",
    "        vocab_size = len(json.dumps(chunked_vocab).encode(\"utf-8\"))\n",
    "        total_size = index_size + vocab_size\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"index\": chunked_index,\n",
    "                \"vocab\": chunked_vocab,\n",
    "                \"index_size\": index_size,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"total_size\": total_size,\n",
    "            }\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal para realizar a an√°lise de hiperpar√¢metros\n",
    "def analyze_hyperparameters(\n",
    "    files_dir, search_terms, chunk_sizes, output_file, progress_file\n",
    "):\n",
    "    results = index_documents_with_different_chunk_sizes(files_dir, chunk_sizes)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f_output, open(\n",
    "        progress_file, \"w\", encoding=\"utf-8\"\n",
    "    ) as f_progress:\n",
    "        for result in results:\n",
    "            avg_search_time = measure_search_time_with_different_chunk_sizes(\n",
    "                result[\"index\"], result[\"vocab\"], search_terms\n",
    "            )\n",
    "            result[\"avg_search_time\"] = avg_search_time\n",
    "\n",
    "            f_output.write(f\"Chunk Size: {result['chunk_size']}\\n\")\n",
    "            f_output.write(f\"Index Size: {result['index_size']} bytes\\n\")\n",
    "            f_output.write(f\"Vocab Size: {result['vocab_size']} bytes\\n\")\n",
    "            f_output.write(f\"Total Size: {result['total_size']} bytes\\n\")\n",
    "            f_output.write(\n",
    "                f\"Average Search Time: {result['avg_search_time']:.6f} segundos\\n\"\n",
    "            )\n",
    "            f_output.write(\"\\n\")\n",
    "\n",
    "            f_progress.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "            f_output.flush()\n",
    "            f_progress.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os par√¢metros\n",
    "files_dir = (\"index_geral.json\", \"vocab_geral.json\")\n",
    "search_terms = [\"educa√ß√£o\", \"linguagem\", \"noticia\", \"ideia\", \"politica\"]\n",
    "chunk_sizes = [50, 100, 200, 500]  # Tamanhos de chunks para testar\n",
    "output_file = \"hyperparameter_analysis.txt\"\n",
    "progress_file = \"hyperparametro_progress.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_hyperparameters(\n",
    "    files_dir, search_terms, chunk_sizes, output_file, progress_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
