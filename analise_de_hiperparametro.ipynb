{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca de √çndice e Consulta de documento\n",
    "\n",
    "**Autor:** Davi J. Leite Santos  \n",
    "**Vers√£o:** 0.0.3  \n",
    "**Data:** 25 de Abril de 2024  \n",
    "**Localiza√ß√£o:** Ribeir√£o das Neves, Minas Gerais - Brasil  \n",
    "\n",
    "## Contato\n",
    "- üè† **Endere√ßo:** Ribeir√£o das Neves, Minas Gerais - Brasil\n",
    "- üìß **Email:** davi.jls@outlook.com\n",
    "- üåê **LinkedIn:** davi-j-leite-santos\n",
    "- üåê **Website:** davijls.com.br\n",
    "\n",
    "## Principais Compet√™ncias\n",
    "- **Ciberseguran√ßa**\n",
    "- **Seguran√ßa da Informa√ß√£o**\n",
    "- **Opera√ß√µes de TI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Baixar os recursos necess√°rios do NLTK\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar o Stemmer para portugu√™s\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun√ß√µes de cria√ß√£o de chunk para a analise de granularidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e vocabul√°rio\n",
    "def load_data(index_file, vocab_file):\n",
    "    with open(index_file, 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar chunks de um documento\n",
    "def create_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para processar arquivo com granularidade ajust√°vel\n",
    "def process_file(file_path, chunk_size, doc_id, vocab, index):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Arquivo n√£o encontrado: {file_path}\")\n",
    "        return\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    chunks = create_chunks(text, chunk_size)\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = nltk.word_tokenize(chunk, language='portuguese')\n",
    "        for pos, token in enumerate(tokens):\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token not in vocab:\n",
    "                vocab[stemmed_token] = len(vocab)\n",
    "            word_id = vocab[stemmed_token]\n",
    "            if word_id not in index:\n",
    "                index[word_id] = {}\n",
    "            if doc_id not in index[word_id]:\n",
    "                index[word_id][doc_id] = []\n",
    "            index[word_id][doc_id].append((chunk_id, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para indexar documentos com diferentes tamanhos de chunks\n",
    "def index_documents_with_different_chunk_sizes(files_dir, chunk_sizes):\n",
    "    results = []\n",
    "    files = sorted(os.listdir(files_dir))\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        vocab = {}\n",
    "        index = {}\n",
    "        for doc_id, file_name in enumerate(files):\n",
    "            file_path = os.path.join(files_dir, file_name)\n",
    "            process_file(file_path, chunk_size, doc_id, vocab, index)\n",
    "        \n",
    "        # Salvar resultados temporariamente para medi√ß√£o\n",
    "        temp_vocab_file = f'temp_vocab_{chunk_size}.json'\n",
    "        temp_index_file = f'temp_index_{chunk_size}.json'\n",
    "        with open(temp_vocab_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab, f)\n",
    "        with open(temp_index_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index, f)\n",
    "        \n",
    "        # Medir o tamanho dos arquivos\n",
    "        vocab_size = os.path.getsize(temp_vocab_file)\n",
    "        index_size = os.path.getsize(temp_index_file)\n",
    "        total_size = index_size + vocab_size\n",
    "        \n",
    "        # Adicionar resultados\n",
    "        results.append({\n",
    "            'chunk_size': chunk_size,\n",
    "            'index_size': index_size,\n",
    "            'vocab_size': vocab_size,\n",
    "            'total_size': total_size,\n",
    "            'index_file': temp_index_file,\n",
    "            'vocab_file': temp_vocab_file,\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para buscar termos no √≠ndice\n",
    "def search_term(index, vocab, term):\n",
    "    stemmed_term = stemmer.stem(term)\n",
    "    if stemmed_term in vocab:\n",
    "        word_id = vocab[stemmed_term]\n",
    "        if str(word_id) in index:\n",
    "            return index[str(word_id)]\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para medir o tempo de pesquisa com diferentes tamanhos de chunks\n",
    "def measure_search_time_with_different_chunk_sizes(index_file, vocab_file, search_terms):\n",
    "    index, vocab = load_data(index_file, vocab_file)\n",
    "    search_times = []\n",
    "    for term in search_terms:\n",
    "        start_time = time.time()\n",
    "        search_term(index, vocab, term)\n",
    "        end_time = time.time()\n",
    "        search_times.append(end_time - start_time)\n",
    "    avg_search_time = sum(search_times) / len(search_times)\n",
    "    return avg_search_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para buscar termos no √≠ndice\n",
    "def search_term(index, vocab, term):\n",
    "    stemmed_term = stemmer.stem(term)\n",
    "    if stemmed_term in vocab:\n",
    "        word_id = vocab[stemmed_term]\n",
    "        if str(word_id) in index:\n",
    "            return index[str(word_id)]\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para salvar o progresso em um arquivo\n",
    "def save_progress(progress_file, progress):\n",
    "    with open(progress_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Progress:\\n\")\n",
    "        for key, value in progress.items():\n",
    "            f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal para realizar a an√°lise de hiperpar√¢metros\n",
    "def analyze_hyperparameters(files_dir, search_terms, chunk_sizes, output_file, progress_file):\n",
    "    progress = {}\n",
    "    try:\n",
    "        # Tenta carregar o progresso anterior\n",
    "        with open(progress_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                key, value = line.strip().split(\": \")\n",
    "                progress[key] = int(value)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    results = index_documents_with_different_chunk_sizes(files_dir, chunk_sizes)\n",
    "    for i, result in enumerate(results):\n",
    "        if 'current_index' not in progress:\n",
    "            progress['current_index'] = i\n",
    "        else:\n",
    "            progress['current_index'] += 1\n",
    "        save_progress(progress_file, progress)\n",
    "        \n",
    "        avg_search_time = measure_search_time_with_different_chunk_sizes(result['index_file'], result['vocab_file'], search_terms)\n",
    "        result['avg_search_time'] = avg_search_time\n",
    "        \n",
    "        # Salvar resultados parciais em um arquivo\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"Chunk Size: {result['chunk_size']}\\n\")\n",
    "            f.write(f\"Index Size: {result['index_size']} bytes\\n\")\n",
    "            f.write(f\"Vocab Size: {result['vocab_size']} bytes\\n\")\n",
    "            f.write(f\"Total Size: {result['total_size']} bytes\\n\")\n",
    "            f.write(f\"Average Search Time: {result['avg_search_time']:.6f} segundos\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # Remover o arquivo de progresso ap√≥s a conclus√£o\n",
    "    os.remove(progress_file) if os.path.exists(progress_file) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os par√¢metros\n",
    "files_dir = '../Motor_de_busca-WebCrawler/sites_visitados'\n",
    "search_terms = ['educa√ß√£o', 'linguagem', 'noticia', 'ideia', 'politica']\n",
    "chunk_sizes = [50, 100, 200, 500]  # Tamanhos de chunks para testar\n",
    "output_file = 'hyperparameter_analysis.txt'\n",
    "progress_file = 'hyperparametro_progress.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo n√£o encontrado: ../Motor_de_busca-WebCrawler/sites_visitados\\blogbvps_wordpress_com]2019]11]22]a-educacao-pela-pratica-da-linguagem-uma-chance-pedagogico-filologica-na-poesia-de-joao-cabral-de-melo-neto-e-na-filosofia-de-paulo-freire-por-rafael-zacca-ufrj].txt\n",
      "Arquivo n√£o encontrado: ../Motor_de_busca-WebCrawler/sites_visitados\\blogger_googleusercontent_com]img]b]R29vZ2xl]AVvXsEhR5ror77j37iSDDHw1FUaSHKnDQwXdDClY8bgoxAaMmPYyALTU6SPeOVqm5b08ifGtF1ZH-TTbBifE9aRN64iJfbTJC5UJ_BDeT_kABv0a9RalSvnvgY3XSXNOdBOePOeGIbk-82iODhU]s337]LOUCO.jpg.txt\n",
      "Arquivo n√£o encontrado: ../Motor_de_busca-WebCrawler/sites_visitados\\capital_sp_gov_br]web]prefeitura-de-sao-paulo]w]prefeitura-oficializa-doa%C3%A7%C3%A3o-da-parque-princesa-isabel-ao-estado-para-viabilizar-a-transforma%C3%A7%C3%A3o-da-regi%C3%A3o-central%C2%A0.txt\n",
      "Arquivo n√£o encontrado: ../Motor_de_busca-WebCrawler/sites_visitados\\epocanegocios_globo_com]podcast]negnews]noticia]2024]04]como-a-vivo-usa-ia-para-melhorar-a-experiencia-do-cliente-reter-funcionarios-e-estabelecer-parcerias-com-grandes-empresas-e-startups_ghtml.txt\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\davim\\AppData\\Local\\Temp\\ipykernel_25756\\1679607890.py\", line 2, in <module>\n",
      "    analyze_hyperparameters(files_dir, search_terms, chunk_sizes, output_file, progress_file)\n",
      "  File \"C:\\Users\\davim\\AppData\\Local\\Temp\\ipykernel_25756\\938647413.py\", line 13, in analyze_hyperparameters\n",
      "    results = index_documents_with_different_chunk_sizes(files_dir, chunk_sizes)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Local\\Temp\\ipykernel_25756\\3009419626.py\", line 11, in index_documents_with_different_chunk_sizes\n",
      "    process_file(file_path, chunk_size, doc_id, vocab, index)\n",
      "  File \"C:\\Users\\davim\\AppData\\Local\\Temp\\ipykernel_25756\\1797409972.py\", line 12, in process_file\n",
      "    stemmed_token = stemmer.stem(token)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\davim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\stem\\rslp.py\", line 120, in stem\n",
      "    word = self.apply_rule(word, 5)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\davim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\stem\\rslp.py\", line -1, in apply_rule\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\davim\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Realizar a an√°lise de hiperpar√¢metros\n",
    "analyze_hyperparameters(files_dir, search_terms, chunk_sizes, output_file, progress_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
