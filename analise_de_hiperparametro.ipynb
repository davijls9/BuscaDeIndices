{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busca de √çndice e Consulta de documento\n",
    "\n",
    "**Autor:** Davi J. Leite Santos  \n",
    "**Vers√£o:** 0.0.3  \n",
    "**Data:** 25 de Abril de 2024  \n",
    "**Localiza√ß√£o:** Ribeir√£o das Neves, Minas Gerais - Brasil  \n",
    "\n",
    "## Contato\n",
    "- üè† **Endere√ßo:** Ribeir√£o das Neves, Minas Gerais - Brasil\n",
    "- üìß **Email:** davi.jls@outlook.com\n",
    "- üåê **LinkedIn:** davi-j-leite-santos\n",
    "- üåê **Website:** davijls.com.br\n",
    "\n",
    "## Principais Compet√™ncias\n",
    "- **Ciberseguran√ßa**\n",
    "- **Seguran√ßa da Informa√ß√£o**\n",
    "- **Opera√ß√µes de TI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\davim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Baixar os recursos necess√°rios do NLTK\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar o Stemmer para portugu√™s\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun√ß√µes de cria√ß√£o de chunk para a analise de granularidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e o vocabul√°rio\n",
    "def load_index(index_file, vocab_file):\n",
    "    with open(index_file, 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar chunks de um documento\n",
    "def create_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para buscar termos no √≠ndice\n",
    "def search_term(index, vocab, term):\n",
    "    stemmed_term = stemmer.stem(term)\n",
    "    if stemmed_term in vocab:\n",
    "        word_id = vocab[stemmed_term]\n",
    "        if str(word_id) in index:\n",
    "            return index[str(word_id)]\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para salvar o progresso em um arquivo\n",
    "def save_progress(progress_file, progress):\n",
    "    with open(progress_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Progress:\\n\")\n",
    "        for key, value in progress.items():\n",
    "            f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar o √≠ndice e o vocabul√°rio\n",
    "def load_index(index_file, vocab_file):\n",
    "    with open(index_file, 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    return index, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para simular a cria√ß√£o de arquivos de √≠ndice e vocabul√°rio\n",
    "def create_temp_files(index_file, vocab_file, chunk_size):\n",
    "    # Criar dados fict√≠cios para o √≠ndice e vocabul√°rio\n",
    "    index_data = {f'word{chunk_size}': [chunk_size, chunk_size+1]}\n",
    "    vocab_data = {f'word{chunk_size}': chunk_size}\n",
    "\n",
    "    # Salvar dados fict√≠cios nos arquivos\n",
    "    with open(index_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(index_data, f)\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para processar arquivo com granularidade ajust√°vel\n",
    "def process_file(file_path, chunk_size, doc_id, vocab, index):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Arquivo n√£o encontrado: {file_path}\")\n",
    "        return\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    chunks = create_chunks(text, chunk_size)\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        tokens = nltk.word_tokenize(chunk, language='portuguese')\n",
    "        for pos, token in enumerate(tokens):\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token not in vocab:\n",
    "                vocab[stemmed_token] = len(vocab)\n",
    "            word_id = vocab[stemmed_token]\n",
    "            if word_id not in index:\n",
    "                index[word_id] = {}\n",
    "            if doc_id not in index[word_id]:\n",
    "                index[word_id][doc_id] = []\n",
    "            index[word_id][doc_id].append((chunk_id, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index_with_chunks(files, chunk_size):\n",
    "    index = {}\n",
    "    vocab = {}\n",
    "    vocab_id = 0\n",
    "    doc_id = 0\n",
    "\n",
    "    for file_path in files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            words = f.read().split()\n",
    "        \n",
    "        chunk_id = 0\n",
    "        while chunk_id * chunk_size < len(words):\n",
    "            chunk = words[chunk_id * chunk_size:(chunk_id + 1) * chunk_size]\n",
    "            for pos, word in enumerate(chunk):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = vocab_id\n",
    "                    vocab_id += 1\n",
    "                term_id = vocab[word]\n",
    "                if term_id not in index:\n",
    "                    index[term_id] = {}\n",
    "                if doc_id not in index[term_id]:\n",
    "                    index[term_id][doc_id] = []\n",
    "                index[term_id][doc_id].append(pos + chunk_id * chunk_size)\n",
    "            chunk_id += 1\n",
    "        doc_id += 1\n",
    "\n",
    "    return index, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para simular a medi√ß√£o do tempo de busca com diferentes tamanhos de chunk\n",
    "def measure_search_time_with_different_chunk_sizes(index, vocab, search_terms):\n",
    "    total_search_time = 0.0\n",
    "    num_searches = len(search_terms)\n",
    "    for term in search_terms:\n",
    "        start_time = time.time()\n",
    "        _ = index.get(term, [])\n",
    "        total_search_time += time.time() - start_time\n",
    "    \n",
    "    avg_search_time = total_search_time / num_searches\n",
    "    return avg_search_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para simular a indexa√ß√£o de documentos com diferentes tamanhos de chunk\n",
    "def index_documents_with_different_chunk_sizes(files_dir, chunk_sizes):\n",
    "    index_file, vocab_file = files_dir\n",
    "    index, vocab = load_index(index_file, vocab_file)\n",
    "    \n",
    "    results = []\n",
    "    for chunk_size in chunk_sizes:\n",
    "        chunked_index = {k: v[:chunk_size] if isinstance(v, list) else v for k, v in index.items()}\n",
    "        chunked_vocab = {k: v for k, v in vocab.items() if len(k) <= chunk_size}\n",
    "        \n",
    "        index_size = len(json.dumps(chunked_index).encode('utf-8'))\n",
    "        vocab_size = len(json.dumps(chunked_vocab).encode('utf-8'))\n",
    "        total_size = index_size + vocab_size\n",
    "        \n",
    "        results.append({\n",
    "            'chunk_size': chunk_size,\n",
    "            'index': chunked_index,\n",
    "            'vocab': chunked_vocab,\n",
    "            'index_size': index_size,\n",
    "            'vocab_size': vocab_size,\n",
    "            'total_size': total_size,\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal para realizar a an√°lise de hiperpar√¢metros\n",
    "def analyze_hyperparameters(files_dir, search_terms, chunk_sizes, output_file, progress_file):\n",
    "    results = index_documents_with_different_chunk_sizes(files_dir, chunk_sizes)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_output, open(progress_file, 'w', encoding='utf-8') as f_progress:\n",
    "        for result in results:\n",
    "            avg_search_time = measure_search_time_with_different_chunk_sizes(result['index'], result['vocab'], search_terms)\n",
    "            result['avg_search_time'] = avg_search_time\n",
    "            \n",
    "            f_output.write(f\"Chunk Size: {result['chunk_size']}\\n\")\n",
    "            f_output.write(f\"Index Size: {result['index_size']} bytes\\n\")\n",
    "            f_output.write(f\"Vocab Size: {result['vocab_size']} bytes\\n\")\n",
    "            f_output.write(f\"Total Size: {result['total_size']} bytes\\n\")\n",
    "            f_output.write(f\"Average Search Time: {result['avg_search_time']:.6f} segundos\\n\")\n",
    "            f_output.write(\"\\n\")\n",
    "            \n",
    "            f_progress.write(json.dumps(result) + \"\\n\")\n",
    "            \n",
    "            f_output.flush()\n",
    "            f_progress.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os par√¢metros\n",
    "files_dir = ('index_geral.json', 'vocab_geral.json')\n",
    "search_terms = ['educa√ß√£o', 'linguagem', 'noticia', 'ideia', 'politica']\n",
    "chunk_sizes = [50, 100, 200, 500]  # Tamanhos de chunks para testar\n",
    "output_file = 'hyperparameter_analysis.txt'\n",
    "progress_file = 'hyperparametro_progress.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_hyperparameters(files_dir, search_terms, chunk_sizes, output_file, progress_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
